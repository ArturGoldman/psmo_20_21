{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прикладная статистика в машинном обучении\n",
    "\n",
    "### Семинар 10: Мультиколлинеарность. Отбор регрессоров. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Мультиколлинеарность.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строгая мультиколлинераность появляется при нарушении условия ТГМ о том, что матрица $X$ является матрицей полного ранга и возникает при наличии линейной зависимости между регрессорами. Однако на практике проблемы возникают и при не строгой мультиколлинеарности (квазимультиколлинеарности), при которой $\\det(X'X) \\approx 0$.\n",
    "\n",
    "В теории: строгая мультиколлинеарность приведёт к тому, что оценки МНК будут получаться неединственными, нестрогая мультиколлинеарность не нарушит ТГМ.\n",
    "\n",
    "На практике: мультиколлинеарность приводит к тому, что: \n",
    "- оценки $\\hat{\\beta}$ становятся неустойчивыми\n",
    "- оценки коэффициентов имеют высокие стандартные ошибки\n",
    "- доверительные интервалы становятся широкими\n",
    "- коэффициенты меняют знак или становятся незначимыми при небольших изменениях данных\n",
    "- t-статистики отдельных коэффициентов малы, но F-статистика на значимость в целом велика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds = sns.load_dataset('diamonds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-80-5cb0f7e5d949>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['sum_xy'] = X['x'] + X['y']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept   -14105.076664\n",
      "x             1863.157850\n",
      "y             -814.091949\n",
      "sum_xy        1049.065901\n",
      "dtype: float64\n",
      "Intercept   -2.326418e+11\n",
      "x            2.326418e+14\n",
      "y            2.326418e+14\n",
      "sum_xy      -2.326418e+14\n",
      "dtype: float64\n",
      "Суммарная квадратичная разность: 162366708248113373337532497920.000000\n"
     ]
    }
   ],
   "source": [
    "# Создадим строгую мультиколлинеарность\n",
    "X = diamonds[['price', 'x', 'y']]\n",
    "X['sum_xy'] = X['x'] + X['y']\n",
    "model1 = smf.ols(data = X, formula='price ~ x + y + sum_xy').fit()\n",
    "\n",
    "# Слегка изменим данные\n",
    "X1 = X.copy()\n",
    "X1['x'] = X1['x'] + 0.001\n",
    "model2 = smf.ols(data = X1, formula='price ~ x + y + sum_xy').fit()\n",
    "\n",
    "# Посмотрим на разницу между коэффициентами\n",
    "print(model1.params)\n",
    "print(model2.params)\n",
    "print('Суммарная квадратичная разность: %f' % (np.sum(np.square(model1.params - model2.params))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept   -14105.076664\n",
      "x             2912.223752\n",
      "y              234.973952\n",
      "dtype: float64\n",
      "Intercept   -14107.988887\n",
      "x             2912.223752\n",
      "y              234.973952\n",
      "dtype: float64\n",
      "Суммарная квадратичная разность: 8.481047\n"
     ]
    }
   ],
   "source": [
    "# Для сравнения: модель без мультиколлинеарности\n",
    "Z = diamonds[['price', 'x', 'y']]\n",
    "model3 = smf.ols(data = Z, formula='price ~ x + y').fit()\n",
    "\n",
    "# Слегка изменим данные\n",
    "Z1 = Z.copy()\n",
    "Z1['x'] = Z1['x'] + 0.001\n",
    "model4= smf.ols(data = Z1, formula='price ~ x + y').fit()\n",
    "\n",
    "# Посмотрим на разницу между коэффициентами\n",
    "print(model3.params)\n",
    "print(model4.params)\n",
    "print('Суммарная квадратичная разность: %f' % (np.sum(np.square(model3.params - model4.params))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.782</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.782</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>6.462e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 16 Nov 2020</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:59:20</td>     <th>  Log-Likelihood:    </th> <td>-4.8265e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 53940</td>      <th>  AIC:               </th>  <td>9.653e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 53936</td>      <th>  BIC:               </th>  <td>9.653e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>-2.326e+11</td> <td> 2.87e+11</td> <td>   -0.811</td> <td> 0.418</td> <td>-7.95e+11</td> <td>  3.3e+11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>         <td> 2.326e+14</td> <td> 2.87e+14</td> <td>    0.811</td> <td> 0.418</td> <td> -3.3e+14</td> <td> 7.95e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>y</th>         <td> 2.326e+14</td> <td> 2.87e+14</td> <td>    0.811</td> <td> 0.418</td> <td> -3.3e+14</td> <td> 7.95e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sum_xy</th>    <td>-2.326e+14</td> <td> 2.87e+14</td> <td>   -0.811</td> <td> 0.418</td> <td>-7.95e+14</td> <td>  3.3e+14</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>18702.724</td> <th>  Durbin-Watson:     </th>  <td>   0.442</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>143426.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.464</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>10.433</td>   <th>  Cond. No.          </th>  <td>8.90e+14</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.4e-23. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.782\n",
       "Model:                            OLS   Adj. R-squared:                  0.782\n",
       "Method:                 Least Squares   F-statistic:                 6.462e+04\n",
       "Date:                Mon, 16 Nov 2020   Prob (F-statistic):               0.00\n",
       "Time:                        22:59:20   Log-Likelihood:            -4.8265e+05\n",
       "No. Observations:               53940   AIC:                         9.653e+05\n",
       "Df Residuals:                   53936   BIC:                         9.653e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept  -2.326e+11   2.87e+11     -0.811      0.418   -7.95e+11     3.3e+11\n",
       "x           2.326e+14   2.87e+14      0.811      0.418    -3.3e+14    7.95e+14\n",
       "y           2.326e+14   2.87e+14      0.811      0.418    -3.3e+14    7.95e+14\n",
       "sum_xy     -2.326e+14   2.87e+14     -0.811      0.418   -7.95e+14     3.3e+14\n",
       "==============================================================================\n",
       "Omnibus:                    18702.724   Durbin-Watson:                   0.442\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           143426.007\n",
       "Skew:                           1.464   Prob(JB):                         0.00\n",
       "Kurtosis:                      10.433   Cond. No.                     8.90e+14\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.4e-23. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Исследуем ковариационную матрицу коэффициентов\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Индикаторы мультиколлинеарности.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Очевидный: посмотреть на $\\mathrm{scorr}(X)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.88443516, 0.8654209 , 0.88042812],\n",
       "       [0.88443516, 1.        , 0.97470148, 0.99354016],\n",
       "       [0.8654209 , 0.97470148, 1.        , 0.99376929],\n",
       "       [0.88042812, 0.99354016, 0.99376929, 1.        ]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(X1.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Condition number.\n",
    "\n",
    "Напоминание: $\\|\\cdot\\|$ -- это норма матрицы, если\n",
    "1. $\\|A\\| \\geq 0$, и если $A = 0$, то $\\|A\\| = 0$.\n",
    "3. $\\|a A\\| = |a| \\|A\\|$\n",
    "4. $\\|A+B\\| \\leq \\|A\\| + \\|B\\|$\n",
    "\n",
    "Некоторые матричные нормы также удовлетворяют свойству субмультипликативности: $\\Vert A B \\Vert \\leq \\Vert A \\Vert \\Vert B \\Vert$.\n",
    "\n",
    "Condtition number определяется как $\\mathrm{K}(A) = \\Vert A \\Vert \\Vert A^{-1} \\Vert$. Можно показать, что в случае $l_2$-нормы для симметричной положительно определённой матрицы $A$ верно:\n",
    "\n",
    "$$\n",
    "K(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.396003693050826e+18"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.cond(X.T @ X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Variance Inflation Factor (VIF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{VIF(X_k)} = \\dfrac{1}{1 - R^2_j},\n",
    "$$\n",
    "где $R^2_j$ -- коэффициент детерминации в регрессии $X_j$ на остальные факторы.\n",
    "\n",
    "Если VIF превышает некоторое пороговое значение (5/10/12$\\ldots$), то $X_j$ сильно коллинеарен прочим регрессорам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_inflation_factor(model.model.exog, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_inflation_factor(model.model.exog, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_inflation_factor(model.model.exog, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Борьба с мультиколлинеарностью.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Ничего не делать: оценки остаются несмещёнными и эффективными среди линейных по $y$ несмещённых оценок.  \n",
    "1. Выбросить часть регрессоров. В этом случае жертвуем несмещённостью оставшихся.\n",
    "2. Регуляризация. В этом случае жертвуем несмещённостью коэффициентов.\n",
    "3. Иногда -- PCA.\n",
    "4. Получить больше наблюдений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Отбор регрессоров.\n",
    "---\n",
    "\n",
    "1. Метод последовательного исключения:\n",
    "    1. Оценить модель.\n",
    "    2. Выбросить наиболее незначимый регрессор.\n",
    "    3. Переоценить модель.\n",
    "    4. Повторять шаги B-C до тех пор, пока все регрессоры не станут значимыми.\n",
    "    \n",
    "    \n",
    "2. Метод последовательного включения:\n",
    "    1. Оценить регрессию $y$ на каждый из $X_1$, $\\ldots$, $X_k$. Выбрать самый значимый: $X_q$.\n",
    "    2. Оценить регерссию $y$ на $X_q$ и на каждый из $X_1$, $\\ldots$, $X_{q-1}$, $X_{q+1}$, $\\ldots$, $X_k$. Выбрать сымый значимый. \n",
    "    3. Повторять до тех пор, пока среди включаемых коэффициентов не останется незначимых."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Если есть $k$ регрессоров и хотим узнать, нужно ли включать все или можно включить $m<k$ регрессоров: F-тест (см. позапрошлый семинар). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Если есть $k$ регрессоров и хотим узнать, не нужно ли было включить ещё какие-то переменные, которых у нас нет: RESET-тест Рамсея.\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "H_0: y = X\\beta + u, \\\\\n",
    "H_1: \\text{есть пропущенные переменные},\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Схема:\n",
    "1. Оценить регрессию $\\hat{y} = X\\hat{\\beta}$.\n",
    "2. Оценить регрессию $y_i = \\beta_0 + \\beta_1X_1 + \\ldots + \\beta_kX_k + a_1\\hat{y}_i^2 + \\ldots + a_m\\hat{y}_i^m + u_i$.\n",
    "3. Проверить гипотезу\n",
    "$$\n",
    "\\begin{cases}\n",
    "H_0: \\forall i : a_i = 0, \\\\\n",
    "H_1: \\sum_i a_i^2 > 0,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "например, при помощи F-теста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import linear_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols('np.log(price) ~ depth + table + x + y + z', data = diamonds).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'statsmodels.stats.contrast.ContrastResults'>\n",
       "<F test: F=array([[7697.00812222]]), p=0.0, df_denom=5.39e+04, df_num=2>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reset(model, use_f = True)\n",
    "# H0 отвергается на 5% уровне значимости!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
